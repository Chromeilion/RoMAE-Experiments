Master address: lrdn0078
Master port: 6000
Machine rank: 15434111
Num processes: 4
Num machines: 1
Running command: accelerate launch     --main_process_ip lrdn0078     --main_process_port 6000     --machine_rank $SLURM_PROCID     --num_processes 4     --num_machines 1     --multi_gpu     --enable_cpu_affinity     --num_cpu_threads_per_process 8     --module     --rdzv_backend c10d     --dynamo_mode default     --mixed_precision bf16     --dynamo_backend inductor      image_net pretrain
W0505 15:32:13.116000 423540 torch/distributed/run.py:758] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: Tracking run with wandb version 0.19.10
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Training:   0%|          | 0/100 [00:00<?, ?it/s]Training:   0%|          | 0/100 [00:00<?, ?it/s]Training:   0%|          | 0/100 [00:00<?, ?it/s]Training:   0%|          | 0/100 [00:00<?, ?it/s][rank3]:W0505 15:32:49.833000 423566 torch/_logging/_internal.py:1130] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0505 15:32:50.629000 423564 torch/_logging/_internal.py:1130] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0505 15:32:51.406000 423565 torch/_logging/_internal.py:1130] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0505 15:32:51.580000 423563 torch/_logging/_internal.py:1130] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:2", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:2", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:2", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:2" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:2" = torch.zeros((1, 49, 49), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:3", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:3", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:3", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:3" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:3" = torch.zeros((1, 49, 49), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        
class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:1", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:1", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:1", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:1" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:1" = torch.zeros((1, 49, 49), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        


class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:2", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:2", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:2", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:2" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:2" = torch.zeros((1, 49, 49), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:3", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:3", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:3", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:3" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:3" = torch.zeros((1, 49, 49), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:1", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:1", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:1", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:1" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:1" = torch.zeros((1, 49, 49), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:2", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:2", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:2", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:2" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:2" = torch.zeros((1, 49, 49), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:3", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:3", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:3", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:3" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:3" = torch.zeros((1, 49, 49), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:1", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:1", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:1", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:1" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:1" = torch.zeros((1, 49, 49), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:2", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:2", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:2", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:2" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:2" = torch.zeros((1, 49, 49), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:3", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:3", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:3", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:3" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:3" = torch.zeros((1, 49, 49), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:1", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:1", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:1", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:1" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:1" = torch.zeros((1, 49, 49), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:2", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:2", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:2", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:2" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:2" = torch.zeros((1, 49, 49), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:3", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:3", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:3", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:3" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:3" = torch.zeros((1, 49, 49), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:1", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:1", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:1", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:1" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:1" = torch.zeros((1, 49, 49), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:2", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:2", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:2", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:2" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:2" = torch.zeros((1, 49, 49), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:3", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:3", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:3", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:3" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:3" = torch.zeros((1, 49, 49), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:1", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:1", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:1", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:1" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:1" = torch.zeros((1, 49, 49), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, True, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        
class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        


class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_encoding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_encoding_buffers_timescale_ = L_pos_encoding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_encoding_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_encoding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:3", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:2", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:1", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:3" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:2" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:1" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_3.reshape(1024, 12, 49, 60);  x_3 = None
        x_4: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_self_prev_positions: "i64[1024, 98][98, 1]cuda:2"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_self_prev_positions: "i64[1024, 98][98, 1]cuda:3"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_self_prev_positions: "i64[1024, 98][98, 1]cuda:1"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_prev_positions: "i64[1024, 98][98, 1]cuda:0"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        
class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_self_prev_positions: "i64[1024, 98][98, 1]cuda:2"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        


class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_self_prev_positions: "i64[1024, 98][98, 1]cuda:3"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_self_prev_positions: "i64[1024, 98][98, 1]cuda:1"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_prev_positions: "i64[1024, 98][98, 1]cuda:0"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_prev_positions: "i64[1024, 98][98, 1]cuda:0", L_positions_: "i64[1024, 98][98, 1]cuda:0"):
        l_self_prev_positions = L_self_prev_positions
        l_positions_ = L_positions_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = l_positions_ == l_self_prev_positions;  l_positions_ = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_prev_positions: "i64[1024, 98][98, 1]cuda:2", L_positions_: "i64[1024, 98][98, 1]cuda:2"):
        l_self_prev_positions = L_self_prev_positions
        l_positions_ = L_positions_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = l_positions_ == l_self_prev_positions;  l_positions_ = l_self_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_prev_positions: "i64[1024, 98][98, 1]cuda:1", L_positions_: "i64[1024, 98][98, 1]cuda:1"):
        l_self_prev_positions = L_self_prev_positions
        l_positions_ = L_positions_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = l_positions_ == l_self_prev_positions;  l_positions_ = l_self_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_prev_positions: "i64[1024, 98][98, 1]cuda:3", L_positions_: "i64[1024, 98][98, 1]cuda:3"):
        l_self_prev_positions = L_self_prev_positions
        l_positions_ = L_positions_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = l_positions_ == l_self_prev_positions;  l_positions_ = l_self_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        
/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xk.view(1024, 49, 12, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:2"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:2"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:2"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:2" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:2" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:2" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:3"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:3"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:1"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:3"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:3" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:3" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:3" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        
class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:1"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        


class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:1"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:1" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:1" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:1" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0", L_xk_: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_pos_emb_prev_positions: "i64[1024, 98][98, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 98][98, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:2", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:2", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:2", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:2" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:2" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:2" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:2" = torch.zeros((1, 196, 196), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:2" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:2", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:2", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:2", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:2" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:2" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:2" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:2" = torch.zeros((1, 196, 196), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:2" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:2", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:2", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:2", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:2" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:2" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:2" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:2" = torch.zeros((1, 196, 196), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:2" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:2", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:2", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:2", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:2" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:2" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:2" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:2" = torch.zeros((1, 196, 196), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:2" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:2", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:2", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:2", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:2" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:2" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:2" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:2" = torch.zeros((1, 196, 196), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:2" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:2", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:2", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:2", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:2", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:2", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:2"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:2" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:2" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:2" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:2" = torch.zeros((1, 196, 196), device = device(type='cuda', index=2));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:2" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:2" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:2" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:2" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:2" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:2" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:2" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:2" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:3", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:3", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:3", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:3" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:3" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:3" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:3" = torch.zeros((1, 196, 196), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:3" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:1", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:1", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:1", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:1" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:1" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:1" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:1" = torch.zeros((1, 196, 196), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:1" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, 196, 196), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:3", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:3", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:3", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:3" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:3" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:3" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:3" = torch.zeros((1, 196, 196), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:3" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:1", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:1", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:1", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:1" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:1" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:1" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:1" = torch.zeros((1, 196, 196), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:1" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, 196, 196), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:3", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:3", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:3", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:3" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:3" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:3" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:3" = torch.zeros((1, 196, 196), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:3" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:1", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:1", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:1", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:1" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:1" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:1" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:1" = torch.zeros((1, 196, 196), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:1" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, 196, 196), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:3", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:3", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:3", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:3" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:3" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:3" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:3" = torch.zeros((1, 196, 196), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:3" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:1", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:1", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:1", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:1" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:1" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:1" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:1" = torch.zeros((1, 196, 196), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:1" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, 196, 196), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:3", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:3", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:3", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:3" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:3" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:3" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:3" = torch.zeros((1, 196, 196), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:3" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:1", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:1", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:1", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:1" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:1" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:1" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:1" = torch.zeros((1, 196, 196), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:1" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, 196, 196), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:3", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:3", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:3", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:3", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:3", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:3"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:3" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:3" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:3" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:3" = torch.zeros((1, 196, 196), device = device(type='cuda', index=3));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:3" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:3" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:3" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:3" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:3" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:3" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:3" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:3" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:1", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:1", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:1", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:1", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:1", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:1"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:1" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:1" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:1" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:1" = torch.zeros((1, 196, 196), device = device(type='cuda', index=1));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:1" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:1" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:1" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:1" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:1" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:1" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:1" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:1" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, 196, 196), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, 196, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, 196, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, 196, 3, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, 196, 60);  x_5 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, True, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:3", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:3" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:3" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:1", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:1" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:1" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:2", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:2" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:2" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:3", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:3" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:3" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:1", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:1" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:1" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:2", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:2" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:2" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:3", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:3" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:3" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:1", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:1" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:1" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:2", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:2" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:2" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:3", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:3", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:3"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:3" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:3" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:1", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:1", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:1"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:1" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:1" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:2", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:2", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:2"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:2" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:2" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:3", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:3" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:3" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:3", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:3" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:3" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:3", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:3" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:3" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:3" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:3" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:3" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:3" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:3" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:3" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:3" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:3" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:2", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:2" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:2" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:1", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:1" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:1" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:2", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:2" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:2" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:1", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:1" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:1" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:2", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:2" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:2" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:2" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:2" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:2" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:2" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:2" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:2" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:2" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:2" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:1", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:1" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:1" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:1" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:1" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:1" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:1" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:1" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:1" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:1" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(x_4, 0.0, True, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:1" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:2", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:2", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:2"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:2" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:2" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:2" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:2", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:2", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:2"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:2" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:2" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:2" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:3", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:3", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:3"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:3" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:3" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:3" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:3", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:3", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:3"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:3" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:3" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:3" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:1", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:1", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:1"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:1" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:1" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:1" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:1", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:1", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:1"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:1" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:1" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:1" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:0", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:0", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:0"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:0" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:0" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:0", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:0", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:0"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:0" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:0" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s3: "Sym(s3)", L_self_prev_positions: "i64[1024, s3][s3, 1]cuda:2", s4: "Sym(s3)", L_positions_: "i64[1024, s3][s3, 1]cuda:2"):
        l_self_prev_positions = L_self_prev_positions
        l_positions_ = L_positions_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size = l_positions_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s3)" = size[1];  size = None
        size_1 = l_self_prev_positions.size()
        getitem_2 = size_1[0];  getitem_2 = None
        getitem_3: "Sym(s3)" = size_1[1];  size_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_1 != getitem_3;  getitem_1 = getitem_3 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, s3][s3, 1]cuda:2" = l_positions_ == l_self_prev_positions;  l_positions_ = l_self_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s3: "Sym(s3)", L_self_prev_positions: "i64[1024, s3][s3, 1]cuda:3", s4: "Sym(s3)", L_positions_: "i64[1024, s3][s3, 1]cuda:3"):
        l_self_prev_positions = L_self_prev_positions
        l_positions_ = L_positions_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size = l_positions_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s3)" = size[1];  size = None
        size_1 = l_self_prev_positions.size()
        getitem_2 = size_1[0];  getitem_2 = None
        getitem_3: "Sym(s3)" = size_1[1];  size_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_1 != getitem_3;  getitem_1 = getitem_3 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, s3][s3, 1]cuda:3" = l_positions_ == l_self_prev_positions;  l_positions_ = l_self_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s3: "Sym(s3)", L_self_prev_positions: "i64[1024, s3][s3, 1]cuda:1", s4: "Sym(s3)", L_positions_: "i64[1024, s3][s3, 1]cuda:1"):
        l_self_prev_positions = L_self_prev_positions
        l_positions_ = L_positions_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size = l_positions_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s3)" = size[1];  size = None
        size_1 = l_self_prev_positions.size()
        getitem_2 = size_1[0];  getitem_2 = None
        getitem_3: "Sym(s3)" = size_1[1];  size_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_1 != getitem_3;  getitem_1 = getitem_3 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, s3][s3, 1]cuda:1" = l_positions_ == l_self_prev_positions;  l_positions_ = l_self_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s3: "Sym(s3)", L_self_prev_positions: "i64[1024, s3][s3, 1]cuda:0", s4: "Sym(s3)", L_positions_: "i64[1024, s3][s3, 1]cuda:0"):
        l_self_prev_positions = L_self_prev_positions
        l_positions_ = L_positions_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size = l_positions_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s3)" = size[1];  size = None
        size_1 = l_self_prev_positions.size()
        getitem_2 = size_1[0];  getitem_2 = None
        getitem_3: "Sym(s3)" = size_1[1];  size_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_1 != getitem_3;  getitem_1 = getitem_3 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, s3][s3, 1]cuda:0" = l_positions_ == l_self_prev_positions;  l_positions_ = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:1", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:1", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:1", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:1", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:1", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:1"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:1" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:1" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:1" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:1" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:3", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:3", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:3", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:3", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:3", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:3"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:3" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:3" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:3" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:3" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:2", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:2", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:2", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:2", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:2", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:2"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:2" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:2" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:2" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:2" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:1", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:1", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:1", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:1"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:1" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:1" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:1", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:1", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:1", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:1"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:1" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:1" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:1", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:1", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:1", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:1"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:1" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:1" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:1" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:1" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:1" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:1" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:1" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:3", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:3", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:3", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:3"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:3" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:3" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:3", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:3", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:3", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:3"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:3" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:3" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:3", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:3", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:3", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:3"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:3" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:3" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:3" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:3" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:3" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:3" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:3" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:2", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:2", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:2", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:2"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:2" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:2" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:0", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:0", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:2", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:2", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:2", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:2"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:2" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:2" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:0", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:0", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:2", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:2", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:2", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:2"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:2" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:2" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:2" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:2" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:2" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:2" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:2" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:0", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:0", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, True, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        
Training:   1%|          | 1/100 [01:24<2:18:57, 84.22s/it]Training:   1%|          | 1/100 [01:24<2:18:58, 84.22s/it]Training:   1%|          | 1/100 [01:24<2:19:05, 84.30s/it]Training:   1%|          | 1/100 [01:24<2:19:13, 84.38s/it]Training:   2%|         | 2/100 [01:39<1:11:39, 43.87s/it]Training:   2%|         | 2/100 [01:39<1:11:43, 43.91s/it]Training:   2%|         | 2/100 [01:41<1:12:46, 44.56s/it]Training:   2%|         | 2/100 [01:41<1:12:47, 44.57s/it]Training:   3%|         | 3/100 [01:51<47:07, 29.15s/it]  Training:   3%|         | 3/100 [01:51<47:10, 29.18s/it]  Training:   3%|         | 3/100 [01:51<46:57, 29.04s/it]  Training:   3%|         | 3/100 [01:51<46:59, 29.07s/it]  Training:   4%|         | 4/100 [02:00<33:56, 21.21s/it]Training:   4%|         | 4/100 [02:00<33:57, 21.22s/it]Training:   4%|         | 4/100 [02:00<33:49, 21.14s/it]Training:   4%|         | 4/100 [02:00<33:50, 21.15s/it]Training:   5%|         | 5/100 [02:07<25:41, 16.23s/it]Training:   5%|         | 5/100 [02:08<25:45, 16.27s/it]Training:   5%|         | 5/100 [02:08<26:04, 16.47s/it]Training:   5%|         | 5/100 [02:08<26:06, 16.48s/it]Training:   6%|         | 6/100 [02:15<21:02, 13.43s/it]Training:   6%|         | 6/100 [02:16<21:04, 13.45s/it]Training:   6%|         | 6/100 [02:16<21:13, 13.55s/it]Training:   6%|         | 6/100 [02:16<21:14, 13.56s/it]Training:   7%|         | 7/100 [02:23<17:37, 11.37s/it]Training:   7%|         | 7/100 [02:23<17:45, 11.46s/it]Training:   7%|         | 7/100 [02:23<17:33, 11.33s/it]Training:   7%|         | 7/100 [02:23<17:34, 11.34s/it]Training:   8%|         | 8/100 [02:30<15:15,  9.95s/it]Training:   8%|         | 8/100 [02:30<15:18,  9.99s/it]Training:   8%|         | 8/100 [02:30<15:10,  9.90s/it]Training:   8%|         | 8/100 [02:30<15:10,  9.90s/it]Training:   9%|         | 9/100 [02:37<13:51,  9.13s/it]Training:   9%|         | 9/100 [02:37<13:53,  9.16s/it]Training:   9%|         | 9/100 [02:37<13:49,  9.11s/it]Training:   9%|         | 9/100 [02:37<13:49,  9.12s/it]Training:  10%|         | 10/100 [02:44<12:39,  8.44s/it]Training:  10%|         | 10/100 [02:44<12:38,  8.42s/it]Training:  10%|         | 10/100 [02:44<12:30,  8.34s/it]Training:  10%|         | 10/100 [02:44<12:30,  8.34s/it]Training:  11%|         | 11/100 [02:50<11:37,  7.83s/it]Training:  11%|         | 11/100 [02:50<11:36,  7.82s/it]Training:  11%|         | 11/100 [02:50<11:33,  7.79s/it]Training:  11%|         | 11/100 [02:50<11:33,  7.79s/it]Training:  12%|        | 12/100 [02:57<10:58,  7.49s/it]Training:  12%|        | 12/100 [02:57<10:58,  7.48s/it]Training:  12%|        | 12/100 [02:57<10:53,  7.43s/it]Training:  12%|        | 12/100 [02:57<10:53,  7.43s/it]Training:  13%|        | 13/100 [03:03<10:21,  7.15s/it]Training:  13%|        | 13/100 [03:03<10:24,  7.18s/it]Training:  13%|        | 13/100 [03:04<10:32,  7.27s/it]Training:  13%|        | 13/100 [03:04<10:32,  7.27s/it]Training:  14%|        | 14/100 [03:10<10:07,  7.07s/it]Training:  14%|        | 14/100 [03:10<10:04,  7.03s/it]Training:  14%|        | 14/100 [03:10<10:11,  7.11s/it]Training:  14%|        | 14/100 [03:10<10:04,  7.03s/it]Training:  15%|        | 15/100 [03:17<09:44,  6.88s/it]Training:  15%|        | 15/100 [03:17<09:47,  6.92s/it]Training:  15%|        | 15/100 [03:17<09:47,  6.91s/it]Training:  15%|        | 15/100 [03:17<09:47,  6.91s/it]Training:  16%|        | 16/100 [03:23<09:31,  6.81s/it]Training:  16%|        | 16/100 [03:23<09:34,  6.84s/it]Training:  16%|        | 16/100 [03:23<09:30,  6.79s/it]Training:  16%|        | 16/100 [03:23<09:30,  6.79s/it]Training:  17%|        | 17/100 [03:30<09:21,  6.76s/it]Training:  17%|        | 17/100 [03:30<09:16,  6.71s/it]Training:  17%|        | 17/100 [03:30<09:16,  6.71s/it]Training:  17%|        | 17/100 [03:30<09:22,  6.78s/it]Training:  18%|        | 18/100 [03:36<09:07,  6.67s/it]Training:  18%|        | 18/100 [03:36<09:07,  6.68s/it]Training:  18%|        | 18/100 [03:37<09:10,  6.71s/it]Training:  18%|        | 18/100 [03:37<09:10,  6.71s/it]Training:  19%|        | 19/100 [03:43<09:04,  6.72s/it]Training:  19%|        | 19/100 [03:43<09:00,  6.67s/it]Training:  19%|        | 19/100 [03:43<09:00,  6.67s/it]Training:  19%|        | 19/100 [03:43<09:04,  6.73s/it]Training:  20%|        | 20/100 [03:50<08:52,  6.66s/it]Training:  20%|        | 20/100 [03:50<08:53,  6.66s/it]Training:  20%|        | 20/100 [03:50<08:50,  6.63s/it]Training:  20%|        | 20/100 [03:50<08:50,  6.63s/it]Training:  21%|        | 21/100 [03:56<08:41,  6.60s/it]Training:  21%|        | 21/100 [03:56<08:41,  6.60s/it]Training:  21%|        | 21/100 [03:57<08:44,  6.64s/it]Training:  21%|        | 21/100 [03:57<08:44,  6.64s/it]Training:  22%|       | 22/100 [04:03<08:35,  6.61s/it]Training:  22%|       | 22/100 [04:03<08:36,  6.62s/it]Training:  22%|       | 22/100 [04:03<08:35,  6.60s/it]Training:  22%|       | 22/100 [04:03<08:35,  6.60s/it]Training:  23%|       | 23/100 [04:09<08:26,  6.58s/it]Training:  23%|       | 23/100 [04:09<08:26,  6.58s/it]Training:  23%|       | 23/100 [04:09<08:24,  6.56s/it]Training:  23%|       | 23/100 [04:09<08:24,  6.55s/it]Training:  24%|       | 24/100 [04:16<08:18,  6.56s/it]Training:  24%|       | 24/100 [04:16<08:18,  6.56s/it]Training:  24%|       | 24/100 [04:16<08:17,  6.54s/it]Training:  24%|       | 24/100 [04:16<08:17,  6.54s/it]Training:  25%|       | 25/100 [04:22<08:07,  6.51s/it]Training:  25%|       | 25/100 [04:22<08:08,  6.51s/it]Training:  25%|       | 25/100 [04:22<08:09,  6.52s/it]Training:  25%|       | 25/100 [04:22<08:09,  6.52s/it]Training:  26%|       | 26/100 [04:29<08:04,  6.54s/it]Training:  26%|       | 26/100 [04:29<08:04,  6.54s/it]Training:  26%|       | 26/100 [04:29<08:03,  6.53s/it]Training:  26%|       | 26/100 [04:29<08:03,  6.53s/it]Training:  27%|       | 27/100 [04:36<07:59,  6.57s/it]Training:  27%|       | 27/100 [04:36<07:59,  6.57s/it]Training:  27%|       | 27/100 [04:36<08:04,  6.64s/it]Training:  27%|       | 27/100 [04:36<08:04,  6.64s/it]Training:  28%|       | 28/100 [04:42<07:56,  6.62s/it]Training:  28%|       | 28/100 [04:42<07:56,  6.62s/it]Training:  28%|       | 28/100 [04:43<08:00,  6.67s/it]Training:  28%|       | 28/100 [04:43<08:00,  6.67s/it]Training:  29%|       | 29/100 [04:49<07:52,  6.65s/it]Training:  29%|       | 29/100 [04:49<07:52,  6.66s/it]Training:  29%|       | 29/100 [04:49<07:54,  6.69s/it]Training:  29%|       | 29/100 [04:49<07:54,  6.69s/it]Training:  30%|       | 30/100 [04:56<07:56,  6.80s/it]Training:  30%|       | 30/100 [04:56<07:56,  6.81s/it]Training:  30%|       | 30/100 [04:56<07:55,  6.79s/it]Training:  30%|       | 30/100 [04:56<07:55,  6.79s/it]Training:  31%|       | 31/100 [05:03<07:46,  6.77s/it]Training:  31%|       | 31/100 [05:03<07:47,  6.78s/it]Training:  31%|       | 31/100 [05:03<07:43,  6.72s/it]Training:  31%|       | 31/100 [05:03<07:43,  6.72s/it]Training:  32%|      | 32/100 [05:10<07:36,  6.71s/it]Training:  32%|      | 32/100 [05:10<07:33,  6.67s/it]Training:  32%|      | 32/100 [05:10<07:36,  6.72s/it]Training:  32%|      | 32/100 [05:10<07:33,  6.67s/it]Training:  33%|      | 33/100 [05:16<07:31,  6.73s/it]Training:  33%|      | 33/100 [05:16<07:28,  6.70s/it]Training:  33%|      | 33/100 [05:16<07:28,  6.70s/it]Training:  33%|      | 33/100 [05:16<07:31,  6.74s/it]Training:  34%|      | 34/100 [05:23<07:23,  6.72s/it]Training:  34%|      | 34/100 [05:23<07:23,  6.72s/it]Training:  34%|      | 34/100 [05:23<07:22,  6.70s/it]Training:  34%|      | 34/100 [05:23<07:22,  6.70s/it]Training:  35%|      | 35/100 [05:30<07:16,  6.72s/it]Training:  35%|      | 35/100 [05:30<07:15,  6.70s/it]Training:  35%|      | 35/100 [05:30<07:16,  6.72s/it]Training:  35%|      | 35/100 [05:30<07:15,  6.70s/it]Training:  36%|      | 36/100 [05:36<07:05,  6.65s/it]Training:  36%|      | 36/100 [05:36<07:06,  6.66s/it]Training:  36%|      | 36/100 [05:36<07:06,  6.67s/it]Training:  36%|      | 36/100 [05:36<07:06,  6.67s/it]Training:  37%|      | 37/100 [05:43<06:59,  6.66s/it]Training:  37%|      | 37/100 [05:43<06:58,  6.65s/it]Training:  37%|      | 37/100 [05:43<06:58,  6.65s/it]Training:  37%|      | 37/100 [05:43<06:59,  6.66s/it]Training:  38%|      | 38/100 [05:49<06:51,  6.64s/it]Training:  38%|      | 38/100 [05:49<06:51,  6.64s/it]Training:  38%|      | 38/100 [05:50<06:51,  6.64s/it]Training:  38%|      | 38/100 [05:50<06:51,  6.64s/it]Training:  39%|      | 39/100 [05:56<06:41,  6.59s/it]Training:  39%|      | 39/100 [05:56<06:43,  6.61s/it]Training:  39%|      | 39/100 [05:56<06:45,  6.65s/it]Training:  39%|      | 39/100 [05:56<06:45,  6.65s/it]Training:  40%|      | 40/100 [06:03<06:38,  6.64s/it]Training:  40%|      | 40/100 [06:03<06:39,  6.65s/it]Training:  40%|      | 40/100 [06:03<06:37,  6.62s/it]Training:  40%|      | 40/100 [06:03<06:37,  6.62s/it]Training:  41%|      | 41/100 [06:09<06:30,  6.62s/it]Training:  41%|      | 41/100 [06:09<06:31,  6.63s/it]Training:  41%|      | 41/100 [06:09<06:30,  6.63s/it]Training:  41%|      | 41/100 [06:09<06:30,  6.63s/it]Training:  42%|     | 42/100 [06:16<06:24,  6.63s/it]Training:  42%|     | 42/100 [06:16<06:23,  6.62s/it]Training:  42%|     | 42/100 [06:16<06:24,  6.64s/it]Training:  42%|     | 42/100 [06:16<06:24,  6.62s/it]Training:  43%|     | 43/100 [06:23<06:17,  6.63s/it]Training:  43%|     | 43/100 [06:23<06:17,  6.63s/it]Training:  43%|     | 43/100 [06:23<06:19,  6.66s/it]Training:  43%|     | 43/100 [06:23<06:19,  6.66s/it]Training:  44%|     | 44/100 [06:29<06:12,  6.65s/it]Training:  44%|     | 44/100 [06:29<06:12,  6.66s/it]Training:  44%|     | 44/100 [06:29<06:14,  6.69s/it]Training:  44%|     | 44/100 [06:29<06:14,  6.69s/it]Training:  45%|     | 45/100 [06:36<06:06,  6.66s/it]Training:  45%|     | 45/100 [06:36<06:06,  6.67s/it]Training:  45%|     | 45/100 [06:36<06:09,  6.73s/it]Training:  45%|     | 45/100 [06:36<06:09,  6.73s/it]Training:  46%|     | 46/100 [06:43<06:01,  6.70s/it]Training:  46%|     | 46/100 [06:43<06:01,  6.70s/it]Training:  46%|     | 46/100 [06:43<06:02,  6.72s/it]Training:  46%|     | 46/100 [06:43<06:02,  6.72s/it]Training:  47%|     | 47/100 [06:50<05:55,  6.71s/it]Training:  47%|     | 47/100 [06:50<05:55,  6.72s/it]Training:  47%|     | 47/100 [06:50<05:54,  6.68s/it]Training:  47%|     | 47/100 [06:50<05:54,  6.68s/it]Training:  48%|     | 48/100 [06:56<05:51,  6.76s/it]Training:  48%|     | 48/100 [06:56<05:48,  6.71s/it]Training:  48%|Train loss: 0.9958744049072266

     | 48/100 [06:56<05:51,  6.76s/it]Training:  48%|     | 48/100 [06:56<05:48,  6.71s/it]Training:  49%|     | 49/100 [07:03<05:43,  6.73s/it]Training:  49%|     | 49/100 [07:03<05:45,  6.77s/it]Training:  49%|     | 49/100 [07:03<05:43,  6.73s/it]Training:  49%|     | 49/100 [07:03<05:44,  6.76s/it]Training:  50%|     | 50/100 [07:10<05:34,  6.69s/it]Training:  50%|     | 50/100 [07:10<05:34,  6.69s/it]Training:  50%|     | 50/100 [07:10<05:33,  6.68s/it]Training:  50%|     | 50/100 [07:10<05:33,  6.68s/it]Training:  51%|     | 51/100 [07:16<05:25,  6.65s/it]Training:  51%|     | 51/100 [07:16<05:25,  6.65s/it]Training:  51%|     | 51/100 [07:16<05:27,  6.67s/it]
Evaluating:   0%|          | 0/2 [00:00<?, ?it/s][A/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:1262: UserWarning: Dynamo does not know how to trace the builtin `torch._C._distributed_c10d.PyCapsule._broadcast_coalesced.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
  torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, False, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, False, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, False, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, False, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, False, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "i64[100352][1]cuda:0", L_self_modules_projection_parameters_weight_: "f32[720, 768][768, 1]cuda:0", L_self_modules_projection_parameters_bias_: "f32[720][1]cuda:0", L_x_: "f32[1024, 49, 768][37632, 768, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_encoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_projection_parameters_weight_ = L_self_modules_projection_parameters_weight_
        l_self_modules_projection_parameters_bias_ = L_self_modules_projection_parameters_bias_
        l_x_ = L_x_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_encoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:335 in torch_dynamo_resume_in_forward_at_335, code: positions = positions[~mask[:, None, ...].expand(-1, npd, -1)].reshape(b, npd, -1)
        positions: "i64[1024, 2, 49][98, 49, 1]cuda:0" = l_stack0_.reshape(1024, 2, -1);  l_stack0_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:340 in torch_dynamo_resume_in_forward_at_335, code: x = self.projection(x)
        x: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_projection_parameters_weight_, l_self_modules_projection_parameters_bias_);  l_x_ = l_self_modules_projection_parameters_weight_ = l_self_modules_projection_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:344 in torch_dynamo_resume_in_forward_at_335, code: x = self.inpt_pos_dropout(self.encoder_inpt_pos_embedding(x, positions))
        x_1: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.nn.functional.dropout(x, 0.0, False, False);  x = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 49, 49][2401, 49, 1]cuda:0" = torch.zeros((1, 49, 49), device = device(type='cuda', index=0));  attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch.rms_norm(x_1, (720,), l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 49, 720][35280, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xq.view(1024, 49, 12, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xk.view(1024, 49, 12, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 49, 12, 60][35280, 720, 60, 1]cuda:0" = xv.view(1024, 49, 12, 60);  xv = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_1: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_2.reshape(1024, 12, -1, 30);  x_2 = None
        x_3: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_1.permute([0, 2, 1, 3]);  reshape_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_2: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem: "i64[1024, 98, 1][98, 1, 1]cuda:0" = reshape_2[(Ellipsis, None)]
        getitem_1: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_encoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_encoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 98, 15][1470, 15, 1]cuda:0" = getitem / getitem_1;  getitem = getitem_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 98, 1, 15][1470, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 98, 12, 15][35280, 30, 2940, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 98, 12, 15][17640, 15, 1470, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 98, 12, 30][35280, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, 98, 30][35280, 30, 360, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 49, 60][35280, 2940, 60, 1]cuda:0" = x_5.reshape(1024, 12, 49, 60);  x_5 = None
        x_6: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout_1: "bf16[1024, 49, 12, 60][35280, 60, 2940, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 12, 49, 60][35280, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_4: "bf16[1024, 12, 98, 30][35280, 2940, 30, 1]cuda:0" = x_7.reshape(1024, 12, -1, 30);  x_7 = None
        x_8: "bf16[1024, 98, 12, 30][35280, 30, 2940, 1]cuda:0" = reshape_4.permute([0, 2, 1, 3]);  reshape_4 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_5: "i64[1024, 98][98, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 98][98, 1]cuda:0" = reshape_5 == reshape_2;  reshape_5 = reshape_2 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(12)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 2*s0, 30][720*s0, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, s0, 60][720*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 12, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(12)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 2*s0, 30][720*s0, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, s0, 60][720*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 12, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(12)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 2*s0, 30][720*s0, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, s0, 60][720*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 12, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[720][1]cuda:0", s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(12)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch.rms_norm(l_x_, (720,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 2*s0, 30][720*s0, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, s0, 60][720*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 12, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_n_kv_heads: "Sym(12)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(720)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 2*s0, 30][720*s0, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, s0, 60][720*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 12, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_n_kv_heads: "Sym(12)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(720)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 2*s0, 30][720*s0, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, s0, 60][720*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 12, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_n_kv_heads: "Sym(12)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(720)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 12, 15][720*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 12, 15][360*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 12, 30][720*s0, 360, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 12, 2*s0, 30][720*s0, 30, 360, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 12, s0, 60][720*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 12, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 12, 60][720*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 12, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s1: "Sym(s1)", s2: "Sym(s2)", L_x_: "bf16[1024, s1, s2, 60][60*s1*s2, 60*s2, 60, 1]cuda:0", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_self_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s1)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s2)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s2, s1, 60][60*s1*s2, 60, 60*s2, 1]cuda:0" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s2, 2*s1, 30][60*s1*s2, 60*s1, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s1, s2, 30][60*s1*s2, 30, 60*s1, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s3)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s3)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s1: "Sym(s1)", s2: "Sym(s2)", L_x_: "bf16[1024, s1, s2, 60][60*s1*s2, 60*s2, 60, 1]cuda:0", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_self_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s1)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s2)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s2, s1, 60][60*s1*s2, 60, 60*s2, 1]cuda:0" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s2, 2*s1, 30][60*s1*s2, 60*s1, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s1, s2, 30][60*s1*s2, 30, 60*s1, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s3)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s3)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s1: "Sym(s1)", L_self_prev_positions: "i64[1024, s1][s1, 1]cuda:0", s2: "Sym(s1)", L_positions_: "i64[1024, s1][s1, 1]cuda:0"):
        l_self_prev_positions = L_self_prev_positions
        l_positions_ = L_positions_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size = l_positions_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s1)" = size[1];  size = None
        size_1 = l_self_prev_positions.size()
        getitem_2 = size_1[0];  getitem_2 = None
        getitem_3: "Sym(s1)" = size_1[1];  size_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_1 != getitem_3;  getitem_1 = getitem_3 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, s1][s1, 1]cuda:0" = l_positions_ == l_self_prev_positions;  l_positions_ = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_n_kv_heads: "Sym(12)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(720)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_n_kv_heads: "Sym(12)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(720)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(720)", L_x_: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[720, 720][720, 1]cuda:0", L_self_n_kv_heads: "Sym(12)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(720)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 720][720*s0, 720, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 12, 60][720*s0, 720, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 12, s0, 60][720*s0, 60, 720, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 12, 2*s0, 30][720*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 12, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 12, 30][720*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0", s3: "Sym(s3)", s4: "Sym(s4)", L_xk_: "bf16[1024, s3, s4, 60][60*s3*s4, 60*s4, 60, 1]cuda:0", s5: "Sym(s5)", L_positions_: "i64[1024, 2, s5][2*s5, s5, 1]cuda:0", s6: "Sym(2*s5)", L_pos_emb_prev_positions: "i64[1024, 2*s5][2*s5, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, False, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s3)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s4)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s4, s3, 60][60*s3*s4, 60, 60*s4, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s4, 2*s3, 30][60*s3*s4, 60*s3, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s3, s4, 30][60*s3*s4, 30, 60*s3, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s5][2*s5, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s5)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s5)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s5][2*s5, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0", s3: "Sym(s3)", s4: "Sym(s4)", L_xk_: "bf16[1024, s3, s4, 60][60*s3*s4, 60*s4, 60, 1]cuda:0", s5: "Sym(s5)", L_positions_: "i64[1024, 2, s5][2*s5, s5, 1]cuda:0", s6: "Sym(2*s5)", L_pos_emb_prev_positions: "i64[1024, 2*s5][2*s5, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, False, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s3)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s4)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s4, s3, 60][60*s3*s4, 60, 60*s4, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s4, 2*s3, 30][60*s3*s4, 60*s3, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s3, s4, 30][60*s3*s4, 30, 60*s3, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s5][2*s5, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s5)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s5)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s5][2*s5, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0", s3: "Sym(s3)", s4: "Sym(s4)", L_xk_: "bf16[1024, s3, s4, 60][60*s3*s4, 60*s4, 60, 1]cuda:0", s5: "Sym(s5)", L_positions_: "i64[1024, 2, s5][2*s5, s5, 1]cuda:0", s6: "Sym(2*s5)", L_pos_emb_prev_positions: "i64[1024, 2*s5][2*s5, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, False, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s3)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s4)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s4, s3, 60][60*s3*s4, 60, 60*s4, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s4, 2*s3, 30][60*s3*s4, 60*s3, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s3, s4, 30][60*s3*s4, 30, 60*s3, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s5][2*s5, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s5)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s5)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s5][2*s5, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(49)", L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:370 in torch_dynamo_resume_in_forward_at_349, code: attn_mask = _get_attn_mask(x.shape, x.device, pad_mask)
        size = x_1.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(196)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, getitem_1, getitem_1), device = device(type='cuda', index=0));  getitem_1 = attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size_1 = rms_norm.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(196)" = size_1[1]
        getitem_5 = size_1[2];  size_1 = getitem_5 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, getitem_4, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, getitem_4, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, getitem_4, 3, 60);  xv = getitem_4 = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xq_1.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(196)" = size_2[1]
        getitem_8 = size_2[2];  getitem_8 = None
        getitem_9 = size_2[3];  size_2 = getitem_9 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_10: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_11: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem_10 / getitem_11;  getitem_10 = getitem_11 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, getitem_7, 60);  x_5 = getitem_7 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(49)", L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:370 in torch_dynamo_resume_in_forward_at_349, code: attn_mask = _get_attn_mask(x.shape, x.device, pad_mask)
        size = x_1.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(196)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, getitem_1, getitem_1), device = device(type='cuda', index=0));  getitem_1 = attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size_1 = rms_norm.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(196)" = size_1[1]
        getitem_5 = size_1[2];  size_1 = getitem_5 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, getitem_4, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, getitem_4, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, getitem_4, 3, 60);  xv = getitem_4 = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xq_1.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(196)" = size_2[1]
        getitem_8 = size_2[2];  getitem_8 = None
        getitem_9 = size_2[3];  size_2 = getitem_9 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_10: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_11: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem_10 / getitem_11;  getitem_10 = getitem_11 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, getitem_7, 60);  x_5 = getitem_7 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(49)", L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:370 in torch_dynamo_resume_in_forward_at_349, code: attn_mask = _get_attn_mask(x.shape, x.device, pad_mask)
        size = x_1.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(196)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, getitem_1, getitem_1), device = device(type='cuda', index=0));  getitem_1 = attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size_1 = rms_norm.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(196)" = size_1[1]
        getitem_5 = size_1[2];  size_1 = getitem_5 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, getitem_4, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, getitem_4, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, getitem_4, 3, 60);  xv = getitem_4 = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xq_1.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(196)" = size_2[1]
        getitem_8 = size_2[2];  getitem_8 = None
        getitem_9 = size_2[3];  size_2 = getitem_9 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_10: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_11: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem_10 / getitem_11;  getitem_10 = getitem_11 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, getitem_7, 60);  x_5 = getitem_7 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(49)", L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:370 in torch_dynamo_resume_in_forward_at_349, code: attn_mask = _get_attn_mask(x.shape, x.device, pad_mask)
        size = x_1.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(196)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, getitem_1, getitem_1), device = device(type='cuda', index=0));  getitem_1 = attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size_1 = rms_norm.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(196)" = size_1[1]
        getitem_5 = size_1[2];  size_1 = getitem_5 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, getitem_4, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, getitem_4, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, getitem_4, 3, 60);  xv = getitem_4 = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xq_1.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(196)" = size_2[1]
        getitem_8 = size_2[2];  getitem_8 = None
        getitem_9 = size_2[3];  size_2 = getitem_9 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_10: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_11: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem_10 / getitem_11;  getitem_10 = getitem_11 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, getitem_7, 60);  x_5 = getitem_7 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(49)", L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:370 in torch_dynamo_resume_in_forward_at_349, code: attn_mask = _get_attn_mask(x.shape, x.device, pad_mask)
        size = x_1.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(196)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, getitem_1, getitem_1), device = device(type='cuda', index=0));  getitem_1 = attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size_1 = rms_norm.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(196)" = size_1[1]
        getitem_5 = size_1[2];  size_1 = getitem_5 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, getitem_4, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, getitem_4, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, getitem_4, 3, 60);  xv = getitem_4 = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xq_1.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(196)" = size_2[1]
        getitem_8 = size_2[2];  getitem_8 = None
        getitem_9 = size_2[3];  size_2 = getitem_9 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_10: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_11: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem_10 / getitem_11;  getitem_10 = getitem_11 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, getitem_7, 60);  x_5 = getitem_7 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(49)", L_stack0_: "bf16[1024, 49, 720][35280, 720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_weight_: "f32[180, 720][720, 1]cuda:0", L_self_modules_encoder_decoder_proj_parameters_bias_: "f32[180][1]cuda:0", L_self_parameters_mask_token_: "f32[1, 1, 180][180, 180, 1]cuda:0", L_m_x_: "f32[1024, 147, 768][112896, 768, 1]cuda:0", L_m_positions_: "i64[1024, 2, 147][294, 147, 1]cuda:0", L_positions_: "i64[1024, 2, 49][98, 49, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_decoder_attn_pos_embedding_buffers_timescale_: "f32[15][1]cuda:0"):
        l_stack0_ = L_stack0_
        l_self_modules_encoder_decoder_proj_parameters_weight_ = L_self_modules_encoder_decoder_proj_parameters_weight_
        l_self_modules_encoder_decoder_proj_parameters_bias_ = L_self_modules_encoder_decoder_proj_parameters_bias_
        l_self_parameters_mask_token_ = L_self_parameters_mask_token_
        l_m_x_ = L_m_x_
        l_m_positions_ = L_m_positions_
        l_positions_ = L_positions_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_
        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_
        l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = L_self_modules_decoder_attn_pos_embedding_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:356 in torch_dynamo_resume_in_forward_at_349, code: x = self.encoder_decoder_proj(x)
        x: "bf16[1024, 49, 180][8820, 180, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_encoder_decoder_proj_parameters_weight_, l_self_modules_encoder_decoder_proj_parameters_bias_);  l_stack0_ = l_self_modules_encoder_decoder_proj_parameters_weight_ = l_self_modules_encoder_decoder_proj_parameters_bias_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:358 in torch_dynamo_resume_in_forward_at_349, code: mask_tokens = self.mask_token.expand(b, m_x.shape[1], -1)
        mask_tokens: "f32[1024, 147, 180][0, 0, 1]cuda:0" = l_self_parameters_mask_token_.expand(1024, 147, -1);  l_self_parameters_mask_token_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:364 in torch_dynamo_resume_in_forward_at_349, code: x = torch.cat([x, mask_tokens], dim=1)
        x_1: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.cat([x, mask_tokens], dim = 1);  x = mask_tokens = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:365 in torch_dynamo_resume_in_forward_at_349, code: positions = torch.cat([positions, m_positions], dim=2)
        positions: "i64[1024, 2, 196][392, 196, 1]cuda:0" = torch.cat([l_positions_, l_m_positions_], dim = 2);  l_positions_ = l_m_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:370 in torch_dynamo_resume_in_forward_at_349, code: attn_mask = _get_attn_mask(x.shape, x.device, pad_mask)
        size = x_1.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(196)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:170 in _get_attn_mask, code: attn_mask = torch.zeros((1, x_shape[1], x_shape[1]), device=device)
        attn_mask: "f32[1, 196, 196][38416, 196, 1]cuda:0" = torch.zeros((1, getitem_1, getitem_1), device = device(type='cuda', index=0));  getitem_1 = attn_mask = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, 196, 180][35280, 180, 1]cuda:0" = torch.rms_norm(x_1, (180,), l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_, 1e-12);  x_1 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size_1 = rms_norm.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(196)" = size_1[1]
        getitem_5 = size_1[2];  size_1 = getitem_5 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, 196, 180][35280, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xq.view(1024, getitem_4, 3, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xk.view(1024, getitem_4, 3, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, 196, 3, 60][35280, 180, 60, 1]cuda:0" = xv.view(1024, getitem_4, 3, 60);  xv = getitem_4 = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xq_1.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(196)" = size_2[1]
        getitem_8 = size_2[2];  getitem_8 = None
        getitem_9 = size_2[3];  size_2 = getitem_9 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_2: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_2.reshape(1024, 3, -1, 30);  x_2 = None
        x_3: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_10: "i64[1024, 392, 1][392, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_11: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_self_modules_decoder_attn_pos_embedding_buffers_timescale_[(None, None, slice(None, None, None))];  l_self_modules_decoder_attn_pos_embedding_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 392, 15][5880, 15, 1]cuda:0" = getitem_10 / getitem_11;  getitem_10 = getitem_11 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 392, 1, 15][5880, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_3, 2, dim = -1);  x_3 = None
        first_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 392, 3, 15][35280, 30, 11760, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 392, 3, 15][17640, 15, 5880, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_4: "bf16[1024, 392, 3, 30][35280, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, 392, 30][35280, 30, 90, 1]cuda:0" = x_4.permute([0, 2, 1, 3]);  x_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, 196, 60][35280, 11760, 60, 1]cuda:0" = x_5.reshape(1024, 3, getitem_7, 60);  x_5 = getitem_7 = None
        x_6: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, 196, 3, 60][35280, 60, 11760, 1]cuda:0" = torch.nn.functional.dropout(x_6, 0.0, False, False);  x_6 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_7: "bf16[1024, 3, 196, 60][35280, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 392, 30][35280, 11760, 30, 1]cuda:0" = x_7.reshape(1024, 3, -1, 30);  x_7 = None
        x_8: "bf16[1024, 392, 3, 30][35280, 30, 11760, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 392][392, 1]cuda:0" = positions.reshape(1024, -1);  positions = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 392][392, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_attention_norm_parameters_weight_: "f32[180][1]cuda:0", s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_attention_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_attention_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_embed_buffers_timescale_: "f32[15][1]cuda:0"):
        l_self_modules_attention_norm_parameters_weight_ = L_self_modules_attention_norm_parameters_weight_
        l_x_ = L_x_
        l_self_modules_attention_modules_wq_parameters_weight_ = L_self_modules_attention_modules_wq_parameters_weight_
        l_self_modules_attention_modules_wk_parameters_weight_ = L_self_modules_attention_modules_wk_parameters_weight_
        l_self_modules_attention_modules_wv_parameters_weight_ = L_self_modules_attention_modules_wv_parameters_weight_
        l_self_modules_attention_n_kv_heads = L_self_modules_attention_n_kv_heads
        l_positions_ = L_positions_
        l_pos_embed_buffers_timescale_ = L_pos_embed_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2929 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        rms_norm: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch.rms_norm(l_x_, (180,), l_self_modules_attention_norm_parameters_weight_, 1e-12);  l_x_ = l_self_modules_attention_norm_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = rms_norm.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wq_parameters_weight_, None);  l_self_modules_attention_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wk_parameters_weight_, None);  l_self_modules_attention_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(rms_norm, l_self_modules_attention_modules_wv_parameters_weight_, None);  rms_norm = l_self_modules_attention_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_modules_attention_n_kv_heads, 60);  xv = getitem_1 = l_self_modules_attention_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_embed_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_embed_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s0)", L_positions_: "i64[1024, 2, s0][2*s0, s0, 1]cuda:0", L_pos_emb_buffers_timescale_: "f32[15][1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_buffers_timescale_ = L_pos_emb_buffers_timescale_
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1]
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:100 in get_sin_cos, code: positions[..., torch.newaxis] / self.timescale[torch.newaxis,
        getitem_7: "i64[1024, 2*s0, 1][2*s0, 1, 1]cuda:0" = reshape_1[(Ellipsis, None)]
        getitem_8: "f32[1, 1, 15][15, 15, 1]cuda:0" = l_pos_emb_buffers_timescale_[(None, None, slice(None, None, None))];  l_pos_emb_buffers_timescale_ = None
        sinusoid_inp: "f32[1024, 2*s0, 15][30*s0, 15, 1]cuda:0" = getitem_7 / getitem_8;  getitem_7 = getitem_8 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:103 in get_sin_cos, code: sinusoid_inp = sinusoid_inp[..., torch.newaxis, :]
        sinusoid_inp_1: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = sinusoid_inp[(Ellipsis, None, slice(None, None, None))];  sinusoid_inp = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:104 in get_sin_cos, code: sin = torch.sin(sinusoid_inp)
        sin: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.sin(sinusoid_inp_1)
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:105 in get_sin_cos, code: cos = torch.cos(sinusoid_inp)
        cos: "f32[1024, 2*s0, 1, 15][30*s0, 15, 15, 1]cuda:0" = torch.cos(sinusoid_inp_1);  sinusoid_inp_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:113 in apply_ndprope, code: first_half, second_half = torch.tensor_split(x, 2, dim=-1)
        tensor_split = torch.tensor_split(x_1, 2, dim = -1);  x_1 = None
        first_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[0]
        second_half: "bf16[1024, 2*s0, 3, 15][180*s0, 30, 60*s0, 1]cuda:0" = tensor_split[1];  tensor_split = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:114 in apply_ndprope, code: first_part = first_half * cos - second_half * sin
        mul: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * cos
        mul_1: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * sin
        first_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul - mul_1;  mul = mul_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:115 in apply_ndprope, code: second_part = second_half * cos + first_half * sin
        mul_2: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = second_half * cos;  second_half = cos = None
        mul_3: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = first_half * sin;  first_half = sin = None
        second_part: "f32[1024, 2*s0, 3, 15][90*s0, 15, 30*s0, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:116 in apply_ndprope, code: out = torch.concatenate([first_part, second_part], dim=-1)
        out: "f32[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = torch.concatenate([first_part, second_part], dim = -1);  first_part = second_part = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:117 in apply_ndprope, code: return out.to(x.dtype)
        x_2: "bf16[1024, 2*s0, 3, 30][180*s0, 90, 30, 1]cuda:0" = out.to(torch.bfloat16);  out = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:132 in forward, code: x = x.permute([0, 2, 1, 3])
        x_3: "bf16[1024, 3, 2*s0, 30][180*s0, 30, 90, 1]cuda:0" = x_2.permute([0, 2, 1, 3]);  x_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:133 in forward, code: x = x.reshape(B, nhead, seq_len, head_dim).permute([0, 2, 1, 3])
        reshape_2: "bf16[1024, 3, s0, 60][180*s0, 60*s0, 60, 1]cuda:0" = x_3.reshape(1024, 3, getitem_4, 60);  x_3 = getitem_4 = None
        x_4: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = reshape_2.permute([0, 2, 1, 3]);  reshape_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in forward, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, 3, 60][180*s0, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(x_4, 0.0, False, False);  x_4 = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_2 = xk_1.size()
        getitem_12 = size_2[0];  getitem_12 = None
        getitem_13: "Sym(s0)" = size_2[1];  getitem_13 = None
        getitem_14 = size_2[2];  getitem_14 = None
        getitem_15 = size_2[3];  size_2 = getitem_15 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x_5: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xk_1.permute([0, 2, 1, 3]);  xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape_3: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x_5.reshape(1024, 3, -1, 30);  x_5 = None
        x_6: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape_3.permute([0, 2, 1, 3]);  reshape_3 = x_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_4: "i64[1024, 2*s0][2*s0, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_3 = reshape_4.size()
        getitem_16 = size_3[0];  getitem_16 = None
        getitem_17: "Sym(2*s0)" = size_3[1];  size_3 = None
        size_4 = reshape_1.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19: "Sym(2*s0)" = size_4[1];  size_4 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_17 != getitem_19;  getitem_17 = getitem_19 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s0][2*s0, 1]cuda:0" = reshape_4 == reshape_1;  reshape_4 = reshape_1 = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:0", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:0", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:0"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:0" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:0" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_x_: "bf16[1024, s0, s1, 60][60*s0*s1, 60*s1, 60, 1]cuda:0", s2: "Sym(s2)", L_positions_: "i64[1024, 2, s2][2*s2, s2, 1]cuda:0", s3: "Sym(2*s2)", L_self_prev_positions: "i64[1024, 2*s2][2*s2, 1]cuda:0"):
        l_x_ = L_x_
        l_positions_ = L_positions_
        l_self_prev_positions = L_self_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s1)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s1, s0, 60][60*s0*s1, 60, 60*s1, 1]cuda:0" = l_x_.permute([0, 2, 1, 3]);  l_x_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s1, 2*s0, 30][60*s0*s1, 60*s0, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s0, s1, 30][60*s0*s1, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s2][2*s2, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s2)" = size_1[1];  size_1 = None
        size_2 = l_self_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s2)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s2][2*s2, 1]cuda:0" = reshape_1 == l_self_prev_positions;  reshape_1 = l_self_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(180)", L_x_: "f32[1024, s0, 180][180*s0, 180, 1]cuda:0", L_self_modules_wq_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wk_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_modules_wv_parameters_weight_: "f32[180, 180][180, 1]cuda:0", L_self_n_kv_heads: "Sym(3)", s3: "Sym(s3)", L_positions_: "i64[1024, 2, s3][2*s3, s3, 1]cuda:0", s4: "Sym(2*s3)", L_pos_emb_prev_positions: "i64[1024, 2*s3][2*s3, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_
        l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_
        l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_
        l_self_n_kv_heads = L_self_n_kv_heads
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:692 in forward, code: bsz, seqlen, _ = x.shape
        size = l_x_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s0)" = size[1]
        getitem_2: "Sym(180)" = size[2];  size = getitem_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:693 in forward, code: xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wq_parameters_weight_, None);  l_self_modules_wq_parameters_weight_ = None
        xk: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wk_parameters_weight_, None);  l_self_modules_wk_parameters_weight_ = None
        xv: "bf16[1024, s0, 180][180*s0, 180, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_wv_parameters_weight_, None);  l_x_ = l_self_modules_wv_parameters_weight_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:695 in forward, code: xq = xq.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xq_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xq.view(1024, getitem_1, l_self_n_kv_heads, 60);  xq = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:696 in forward, code: xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xk_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xk.view(1024, getitem_1, l_self_n_kv_heads, 60);  xk = xk_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:697 in forward, code: xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        xv_1: "bf16[1024, s0, 3, 60][180*s0, 180, 60, 1]cuda:0" = xv.view(1024, getitem_1, l_self_n_kv_heads, 60);  xv = getitem_1 = l_self_n_kv_heads = xv_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size_1 = xq_1.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4: "Sym(s0)" = size_1[1];  getitem_4 = None
        getitem_5 = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, 3, s0, 60][180*s0, 60, 180, 1]cuda:0" = xq_1.permute([0, 2, 1, 3]);  xq_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, 3, 2*s0, 30][180*s0, 60*s0, 30, 1]cuda:0" = x.reshape(1024, 3, -1, 30);  x = None
        x_1: "bf16[1024, 2*s0, 3, 30][180*s0, 30, 60*s0, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s3][2*s3, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_2 = reshape_1.size()
        getitem_7 = size_2[0];  getitem_7 = None
        getitem_8: "Sym(2*s3)" = size_2[1];  size_2 = None
        size_3 = l_pos_emb_prev_positions.size()
        getitem_9 = size_3[0];  getitem_9 = None
        getitem_10: "Sym(2*s3)" = size_3[1];  size_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_8 != getitem_10;  getitem_8 = getitem_10 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s3][2*s3, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:0", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:0", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, False, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:0", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:0", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, False, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", s1: "Sym(s1)", L_stack1_: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0", s2: "Sym(s2)", s3: "Sym(s3)", L_xk_: "bf16[1024, s2, s3, 60][60*s2*s3, 60*s3, 60, 1]cuda:0", s4: "Sym(s4)", L_positions_: "i64[1024, 2, s4][2*s4, s4, 1]cuda:0", s5: "Sym(2*s4)", L_pos_emb_prev_positions: "i64[1024, 2*s4][2*s4, 1]cuda:0"):
        l_stack1_ = L_stack1_
        l_xk_ = L_xk_
        l_positions_ = L_positions_
        l_pos_emb_prev_positions = L_pos_emb_prev_positions
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/model.py:700 in torch_dynamo_resume_in_forward_at_700, code: xq, xk = self.pos_dropout(pos_emb(xq, positions)), self.pos_dropout(pos_emb(xk, positions))
        dropout: "bf16[1024, s0, s1, 60][60*s0*s1, 60, 60*s0, 1]cuda:0" = torch.nn.functional.dropout(l_stack1_, 0.0, False, False);  l_stack1_ = dropout = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:127 in forward, code: B, seq_len, nhead, head_dim = x.shape
        size = l_xk_.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s2)" = size[1];  getitem_1 = None
        getitem_2: "Sym(s3)" = size[2]
        getitem_3 = size[3];  size = getitem_3 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:129 in forward, code: x = x.permute([0, 2, 1, 3])
        x: "bf16[1024, s3, s2, 60][60*s2*s3, 60, 60*s3, 1]cuda:0" = l_xk_.permute([0, 2, 1, 3]);  l_xk_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:130 in forward, code: x = x.reshape(B, nhead, -1, self.axis_dim).permute([0, 2, 1, 3])
        reshape: "bf16[1024, s3, 2*s2, 30][60*s2*s3, 60*s2, 30, 1]cuda:0" = x.reshape(1024, getitem_2, -1, 30);  x = getitem_2 = None
        x_1: "bf16[1024, 2*s2, s3, 30][60*s2*s3, 30, 60*s2, 1]cuda:0" = reshape.permute([0, 2, 1, 3]);  reshape = x_1 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:131 in forward, code: x = self.apply_ndprope(x, positions.reshape(B, -1))
        reshape_1: "i64[1024, 2*s4][2*s4, 1]cuda:0" = l_positions_.reshape(1024, -1);  l_positions_ = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:96 in get_sin_cos, code: if positions.shape == self.prev_positions.shape:
        size_1 = reshape_1.size()
        getitem_4 = size_1[0];  getitem_4 = None
        getitem_5: "Sym(2*s4)" = size_1[1];  size_1 = None
        size_2 = l_pos_emb_prev_positions.size()
        getitem_6 = size_2[0];  getitem_6 = None
        getitem_7: "Sym(2*s4)" = size_2[1];  size_2 = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:93 in list_cmp, code: if a != b:
        ne: "Sym(False)" = getitem_5 != getitem_7;  getitem_5 = getitem_7 = ne = None
        
         # File: /leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/positional_embeddings.py:97 in get_sin_cos, code: if torch.all(positions == self.prev_positions):
        eq: "b8[1024, 2*s4][2*s4, 1]cuda:0" = reshape_1 == l_pos_emb_prev_positions;  reshape_1 = l_pos_emb_prev_positions = None
        all_1: "b8[][]cuda:0" = torch.all(eq);  eq = all_1 = None
        

Evaluating:  50%|     | 1/2 [00:45<00:45, 45.44s/it][A
Evaluating: 100%|| 2/2 [00:45<00:00, 18.91s/it][AEvaluating: 100%|| 2/2 [00:45<00:00, 22.94s/it]
Training:  51%|     | 51/100 [08:03<16:55, 20.71s/it]Training:  52%|    | 52/100 [08:09<13:07, 16.41s/it][rank2]:[E505 15:49:45.666466849 ProcessGroupNCCL.cpp:632] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=710, OpType=BROADCAST, NumelIn=5056, NumelOut=5056, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
[rank2]:[E505 15:49:45.668981689 ProcessGroupNCCL.cpp:2268] [PG ID 0 PG GUID 0(default_pg) Rank 2]  failure detected by watchdog at work sequence id: 710 PG status: last enqueued work: 710, last completed work: 709
[rank2]:[E505 15:49:45.668994591 ProcessGroupNCCL.cpp:670] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E505 15:49:45.669022573 ProcessGroupNCCL.cpp:2103] [PG ID 0 PG GUID 0(default_pg) Rank 2] First PG on this rank to signal dumping.
[rank1]:[E505 15:49:45.694392437 ProcessGroupNCCL.cpp:632] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=710, OpType=BROADCAST, NumelIn=5056, NumelOut=5056, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
[rank1]:[E505 15:49:45.694512286 ProcessGroupNCCL.cpp:2268] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 710 PG status: last enqueued work: 710, last completed work: 709
[rank1]:[E505 15:49:45.694518186 ProcessGroupNCCL.cpp:670] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E505 15:49:45.694546726 ProcessGroupNCCL.cpp:2103] [PG ID 0 PG GUID 0(default_pg) Rank 1] First PG on this rank to signal dumping.
[rank3]:[E505 15:49:45.695772514 ProcessGroupNCCL.cpp:632] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=710, OpType=BROADCAST, NumelIn=5056, NumelOut=5056, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
[rank3]:[E505 15:49:45.695888940 ProcessGroupNCCL.cpp:2268] [PG ID 0 PG GUID 0(default_pg) Rank 3]  failure detected by watchdog at work sequence id: 710 PG status: last enqueued work: 710, last completed work: 709
[rank3]:[E505 15:49:45.695896287 ProcessGroupNCCL.cpp:670] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E505 15:49:45.695925326 ProcessGroupNCCL.cpp:2103] [PG ID 0 PG GUID 0(default_pg) Rank 3] First PG on this rank to signal dumping.
[rank2]:[E505 15:49:45.764590625 ProcessGroupNCCL.cpp:1743] [PG ID 0 PG GUID 0(default_pg) Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 710, last completed NCCL work: 709.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E505 15:49:45.764607145 ProcessGroupNCCL.cpp:1743] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 710, last completed NCCL work: 709.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E505 15:49:45.764673308 ProcessGroupNCCL.cpp:1533] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank2]:[E505 15:49:45.764794746 ProcessGroupNCCL.cpp:1533] [PG ID 0 PG GUID 0(default_pg) Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
Training:  51%|     | 51/100 [17:16<16:36, 20.33s/it]
Training:  51%|     | 51/100 [17:16<16:36, 20.33s/it]Training:  51%|     | 51/100 [17:16<16:36, 20.33s/it]

[rank3]: Traceback (most recent call last):
[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA-Experiments/image_net/image_net/__main__.py", line 35, in <module>
[rank3]:     args.func(args)
[rank3]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA-Experiments/image_net/image_net/__main__.py", line 10, in run_pretrain
[rank3]:     pretrain.pretrain()
[rank3]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA-Experiments/image_net/image_net/pretrain.py", line 36, in pretrain
[rank3]:     trainer.train(
[rank3]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/trainer.py", line 212, in train
[rank3]:     for modelargs in train_dataloader:
[rank3]:   File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/data_loader.py", line 559, in __iter__
[rank3]:     synchronize_rng_states(self.rng_types, self.synchronized_generator)
[rank3]:   File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/utils/random.py", line 156, in synchronize_rng_states
[rank3]:     synchronize_rng_state(RNGType(rng_type), generator=generator)
[rank3]:   File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/utils/random.py", line 151, in synchronize_rng_state
[rank3]:     generator.set_state(rng_state)
[rank3]: RuntimeError: Invalid mt19937 state
[rank1]: Traceback (most recent call last):
[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:   File "<frozen runpy>", line 88, in _run_code
[rank1]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA-Experiments/image_net/image_net/__main__.py", line 35, in <module>
[rank1]:     args.func(args)
[rank1]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA-Experiments/image_net/image_net/__main__.py", line 10, in run_pretrain
[rank1]:     pretrain.pretrain()
[rank1]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA-Experiments/image_net/image_net/pretrain.py", line 36, in pretrain
[rank1]:     trainer.train(
[rank1]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/trainer.py", line 212, in train
[rank1]:     for modelargs in train_dataloader:
[rank1]:   File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/data_loader.py", line 559, in __iter__
[rank1]:     synchronize_rng_states(self.rng_types, self.synchronized_generator)
[rank1]:   File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/utils/random.py", line 156, in synchronize_rng_states
[rank1]:     synchronize_rng_state(RNGType(rng_type), generator=generator)
[rank1]:   File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/utils/random.py", line 151, in synchronize_rng_state
[rank1]:     generator.set_state(rng_state)
[rank1]: RuntimeError: Invalid mt19937 state
[rank2]: Traceback (most recent call last):
[rank2]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank2]:   File "<frozen runpy>", line 88, in _run_code
[rank2]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA-Experiments/image_net/image_net/__main__.py", line 35, in <module>
[rank2]:     args.func(args)
[rank2]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA-Experiments/image_net/image_net/__main__.py", line 10, in run_pretrain
[rank2]:     pretrain.pretrain()
[rank2]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA-Experiments/image_net/image_net/pretrain.py", line 36, in pretrain
[rank2]:     trainer.train(
[rank2]:   File "/leonardo/home/userexternal/mdelosri/imageNet/RoMA/roma/trainer.py", line 212, in train
[rank2]:     for modelargs in train_dataloader:
[rank2]:   File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/data_loader.py", line 559, in __iter__
[rank2]:     synchronize_rng_states(self.rng_types, self.synchronized_generator)
[rank2]:   File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/utils/random.py", line 156, in synchronize_rng_states
[rank2]:     synchronize_rng_state(RNGType(rng_type), generator=generator)
[rank2]:   File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/utils/random.py", line 151, in synchronize_rng_state
[rank2]:     generator.set_state(rng_state)
[rank2]: RuntimeError: Invalid mt19937 state
[rank0]:[E505 15:49:45.812245281 ProcessGroupNCCL.cpp:1682] [PG ID 0 PG GUID 0(default_pg) Rank 0] Observed flight recorder dump signal from another rank via TCPStore.
[rank0]:[E505 15:49:45.818280358 ProcessGroupNCCL.cpp:1743] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from  rank 3 and we will try our best to dump the debug info. Last enqueued NCCL work: 723, last completed NCCL work: 711.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E505 15:49:45.818391117 ProcessGroupNCCL.cpp:1533] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank3]:[E505 15:49:46.668948190 ProcessGroupNCCL.cpp:1743] [PG ID 0 PG GUID 0(default_pg) Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 710, last completed NCCL work: 709.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[E505 15:49:46.669211987 ProcessGroupNCCL.cpp:1533] [PG ID 0 PG GUID 0(default_pg) Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E505 15:50:38.596599416 ProcessGroupNCCL.cpp:632] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=712, OpType=ALLREDUCE, NumelIn=268968, NumelOut=268968, Timeout(ms)=600000) ran for 600067 milliseconds before timing out.
[rank0]:[E505 15:50:38.596735672 ProcessGroupNCCL.cpp:2268] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 712 PG status: last enqueued work: 723, last completed work: 711
[rank0]:[E505 15:50:38.596742322 ProcessGroupNCCL.cpp:670] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E505 15:50:38.698044232 ProcessGroupNCCL.cpp:684] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E505 15:50:38.698057178 ProcessGroupNCCL.cpp:698] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E505 15:50:38.698383203 ProcessGroupNCCL.cpp:684] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E505 15:50:38.698396667 ProcessGroupNCCL.cpp:698] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E505 15:50:38.698403956 ProcessGroupNCCL.cpp:684] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E505 15:50:38.698416088 ProcessGroupNCCL.cpp:698] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E505 15:50:38.702429961 ProcessGroupNCCL.cpp:1896] [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=710, OpType=BROADCAST, NumelIn=5056, NumelOut=5056, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x14d2563e65e8 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x14d257701a1d in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0xc80 (0x14d2577037a0 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x14d257704ead in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc2ba3 (0x14d24752eba3 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x14d2b58431ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x14d2b4d24e73 in /lib64/libc.so.6)

[rank3]:[E505 15:50:38.702434597 ProcessGroupNCCL.cpp:1896] [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=710, OpType=BROADCAST, NumelIn=5056, NumelOut=5056, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x148682d075e8 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x148684022a1d in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0xc80 (0x1486840247a0 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x148684025ead in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc2ba3 (0x148673e4fba3 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x1486e21641ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x1486e1645e73 in /lib64/libc.so.6)

[rank1]:[E505 15:50:38.702436658 ProcessGroupNCCL.cpp:1896] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=710, OpType=BROADCAST, NumelIn=5056, NumelOut=5056, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x148bf50df5e8 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x148bf63faa1d in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0xc80 (0x148bf63fc7a0 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x148bf63fdead in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc2ba3 (0x148be6227ba3 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x148c5453c1ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x148c53a1de73 in /lib64/libc.so.6)

terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::DistBackendError'
c10::DistBackendErrorterminate called after throwing an instance of ''
c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=710, OpType=BROADCAST, NumelIn=5056, NumelOut=5056, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x14d2563e65e8 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x14d257701a1d in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0xc80 (0x14d2577037a0 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x14d257704ead in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc2ba3 (0x14d24752eba3 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x14d2b58431ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x14d2b4d24e73 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x14d2563e65e8 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x11b4a6e (0x14d2576d3a6e in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xe07bed (0x14d257326bed in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xc2ba3 (0x14d24752eba3 in /lib64/libstdc++.so.6)
frame #4: <unknown function> + 0x81ca (0x14d2b58431ca in /lib64/libpthread.so.0)
frame #5: clone + 0x43 (0x14d2b4d24e73 in /lib64/libc.so.6)

  what():  [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=710, OpType=BROADCAST, NumelIn=5056, NumelOut=5056, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x148682d075e8 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x148684022a1d in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0xc80 (0x1486840247a0 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x148684025ead in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc2ba3 (0x148673e4fba3 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x1486e21641ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x1486e1645e73 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x148682d075e8 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x11b4a6e (0x148683ff4a6e in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xe07bed (0x148683c47bed in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xc2ba3 (0x148673e4fba3 in /lib64/libstdc++.so.6)
frame #4: <unknown function> + 0x81ca (0x1486e21641ca in /lib64/libpthread.so.0)
frame #5: clone + 0x43 (0x1486e1645e73 in /lib64/libc.so.6)

  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=710, OpType=BROADCAST, NumelIn=5056, NumelOut=5056, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x148bf50df5e8 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x148bf63faa1d in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0xc80 (0x148bf63fc7a0 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x148bf63fdead in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc2ba3 (0x148be6227ba3 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x148c5453c1ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x148c53a1de73 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x148bf50df5e8 in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x11b4a6e (0x148bf63cca6e in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xe07bed (0x148bf601fbed in /leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xc2ba3 (0x148be6227ba3 in /lib64/libstdc++.so.6)
frame #4: <unknown function> + 0x81ca (0x148c5453c1ca in /lib64/libpthread.so.0)
frame #5: clone + 0x43 (0x148c53a1de73 in /lib64/libc.so.6)

W0505 15:50:38.775000 423540 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 423563 closing signal SIGTERM
W0505 15:50:38.788000 423540 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 423565 closing signal SIGTERM
W0505 15:50:38.789000 423540 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 423566 closing signal SIGTERM
E0505 15:50:39.560000 423540 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: -6) local_rank: 1 (pid: 423564) of binary: /leonardo/home/userexternal/mdelosri/imageNet/.venv/bin/python
Traceback (most recent call last):
  File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1204, in launch_command
    multi_gpu_launcher(args)
  File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/accelerate/commands/launch.py", line 825, in multi_gpu_launcher
    distrib_run.run(args)
  File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/home/userexternal/mdelosri/imageNet/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
image_net FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-05_15:50:38
  host      : lrdn0078.leonardo.local
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 423564)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 423564
=======================================================
srun: error: lrdn0078: task 0: Exited with exit code 1
